{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 04-B - Retrieval Augmented Generation (RAG) for Unstructured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Businesses have a lot of proprietary information that needs to be taken into account when answering user's questions - these cannot always be answered through the data that the GPT models have been trained on. \n",
    "\n",
    "In the last notebook, we worked with structured data primarily. A lot of the time, your enterprise data is not limited to just structured formats like CSV files or SQL tables. It may also include unstructured data like PDF documents or images. In fact, your individual documents could have both unstructured and structured data built into them. Extracting information from these diverse formats in a comprehensible manner presents a challenge. Tools like Azure Form Recognizer enable the extraction of data from unstructured sources such as forms or documents. Once the data is extracted into a structured JSON format, then Cognitive Search can be utilized to consolidate the entire information from different data types into indexes, facilitating the retrieval of relevant documents.\n",
    "\n",
    "In this notebook, we will walk you through a use case of Retrieval Augmented Generation (RAG) that involves working with unstructured data. The RAG approach combines various technologies to enhance the quality and relevance of generated outputs. We will leverage Azure Form Recognizer to process complex documents, utilizing the layout API to extract text and tables effectively. We will utilize Azure Cognitive Search to create an index by configuring semantic search capabilities, enabling the retrieval of relevant document pages. Additionally, embeddings will be incorporated to retrieve content that is more closely aligned with the user's question. Finally, Azure OpenAI's ChatGPT model will utilize the extracted content to generate a more meaningful answer. It is important to emphasize that this grounding process follows the RAG pattern mentioned in the previous notebook and helps eliminate inaccuracies in the generated responses.\n",
    "\n",
    "Your goals for this challenge are to read through this notebook, run each code block, observe the results, and then be able to answer the questions posed in the student guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken==0.9.0\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/vscode/.local/lib/python3.11/site-packages (from tiktoken==0.9.0) (2026.2.19)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/vscode/.local/lib/python3.11/site-packages (from tiktoken==0.9.0) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.9.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.9.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.9.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.9.0) (2026.1.4)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.12.0\n",
      "    Uninstalling tiktoken-0.12.0:\n",
      "      Successfully uninstalled tiktoken-0.12.0\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tiktoken==0.9.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "# Initialize the Azure OpenAI client for the latest version\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model = os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The path in the code cell below is referring to the `/data/unstructured/raw` folder. You may need to update this path if you are running this notebook from a different location then from where you extracted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= '../data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = '../data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"DOCUMENT_INTELLIGENCE_ENDPOINT\"]\n",
    "\n",
    "# Use Entra ID authentication instead of API key\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=credential\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract the data from our unstructured data into a more readable format for the model to understand. The Form Recognizer tool helps us do so by leveraging the prebuilt layout models. Here, we primarily are working with PDFs but we could also have JPG and PNG formats that the form recognizer tool also supports.\n",
    "\n",
    "For each document, we want to specify the way information is being extracted. For example in this use case, each document has many pages. To keep track of the pages, we store them in page_number. We also want to extract the content for each page and drop it in a page_context field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf  write output to ../data/unstructured/extracted/Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.json\n",
      "Processing file: AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf  write output to ../data/unstructured/extracted/AutoPrompt_Eliciting_Knowledge_From_LanguageModels.json\n",
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf  write output to ../data/unstructured/extracted/Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.json\n",
      "Processing file: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf  write output to ../data/unstructured/extracted/Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.json\n",
      "Processing file: Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf  write output to ../data/unstructured/extracted/Generated_Knowledge_Prompting_for_Commonsense_Reasoning.json\n",
      "Processing file: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf  write output to ../data/unstructured/extracted/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.json\n",
      "Processing file: LLMs_are_Human-Level_Prompt_Engineers.pdf  write output to ../data/unstructured/extracted/LLMs_are_Human-Level_Prompt_Engineers.json\n",
      "Processing file: Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf  write output to ../data/unstructured/extracted/Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.json\n"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More About our data\n",
    "\n",
    "For this walkthrough, we will take a look at various Research Papers on LLM topics in PDF documents. This includes topics like autoprompting, chain of thought prompting, precise zero shot dense retrival, and more. This dataset contains various unstructured formats such as text, tables, graphs, and formulas.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The relevant schema for our work today consists of \n",
    "\n",
    "- document_id\n",
    "- document_name\n",
    "- file_path\n",
    "- page_number\n",
    "- page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'document_id':page_content['filename'].split('\\\\')[-1].split('.')[0] + '-' + str(page['page_number']),\n",
    "                'document_name':page_content['filename'].split('\\\\')[-1],\n",
    "                'file_path':page_content['filename'],              \n",
    "                'page_number':page['page_number'],\n",
    "                'page_text':page['page_content']\n",
    "            }\n",
    "            for page in page_content['content']\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': '-2',\n",
       " 'document_name': '../data/unstructured/raw/LLMs_are_Human-Level_Prompt_Engineers.pdf',\n",
       " 'file_path': '../data/unstructured/raw/LLMs_are_Human-Level_Prompt_Engineers.pdf',\n",
       " 'page_number': 2,\n",
       " 'page_text': \"Instruction Approximate Inference using LLMs Model Input I instructed my friend to <M>. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs: Input: Sentence 1: The dinosaurs became extinct. Sentence 2: A large object hit the Earth. Output: A large object hit the Earth. ... 0.8 0.76 1.75 0 71 -- Human Prompt Engineer 0.63 0.65 0.61 0.59 0.6- 0.57 Interquartile Mean Zero-Shot Performance 0.4 0.40 Input: Sentence 1: The company's posted strong earnings. Sentence 2: The company's stock went up. Output: The company's posted strong earnings. Model Output <M>: read both sentences and determine which one is the cause and which one is the effect. Choose the sentence that is the cause and write it as the output. 0.2 - Demonstrations 0.03 0.03 0.01 0.01 0.02 0.01 350M 1.3B 6.7B 175B Greedy (GPT-3) Greedy (InstructGPT) 0.03 0 350M 1.3B 6.7B 175B 350M 1.3B 6.7B 175B APE (GPT-3) 350M 1.3B 6.7B 175B APE (InstructGPT) (a) (b) LLMs as inference models (c) Interquartile mean across 24 tasks Figure 1: (a) Natural language program synthesis finds an appropriate instruction (the program) that generates the observed demonstrations when executed by the model. We frame this as a black-box optimization problem guided by an inference procedure. (b) We use LLMs as inference models to fill in the blank; our algorithm involves a search over candidates proposed by the inference models. (c) As measured by the interquartile mean across the 24 NLP tasks introduced by Honovich et al. (2022), APE is able to surpass human performance when using the InstructGPT model (Ouyang et al., 2022). natural language program synthesis and propose to address it as a black-box optimization problem using LLMs to generate and search over heuristically viable candidate solutions. In doing so, we leverage the generalist capabilities of LLMs in three ways. First, we use an LLM as an inference model (Ellis et al., 2021; Honovich et al., 2022) to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. Next, we guide the search process by computing a score for each instruction under the LLM we seek to control. Finally, we propose an iterative Monte Carlo search method where LLMs improve the best candidates by proposing semantically similar instruction variants. Intuitively, our algorithm asks LLMs to generate a set of instruction candidates based on demonstrations and then asks them to assess which instructions are more promising. We call our algorithm Automatic Prompt Engineer (APE). Our main contributions are: • We frame instruction generation as natural language program synthesis, formulate it as a black-box optimization problem guided by LLMs, and propose both a naive and an iterative Monte Carlo search methods to approximate the solution. • Our proposed method, APE, achieves human-level performance on zero-shot learning with model-generated instructions on 19/24 NLP tasks. • We provide extensive qualitative and quantitative analyses exploring various facets of APE, and demonstrate applications of APE for improving few-shot learning and steering LLMs toward desired behaviors such as truthfulness and/or informativeness. 2 RELATED WORK Large Language Models Scaling up transformer-based language models in terms of model size, training data, and training compute has been shown to predictably improve performance on a wide range of downstream NLP tasks (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). Many emergent abilities (Wei et al., 2022a) of LLMs have been discovered as a result of this scaling, including few-shot in-context learning, zero-shot problem solving, chain of thought reasoning, instruction following, and instruction induction (Cobbe et al., 2021; Wei et al., 2022b; Kojima et al., 2022; Sanh et al., 2022; Wei et al., 2021; Ouyang et al., 2022; Honovich et al., 2022). In this paper, we view LLMs as black-box computers that execute programs specified by natural language instructions and investigate how to control an LLM’s behavior using model-generated instructions. Prompt Engineering Prompting offers a natural and intuitive interface for humans to interact with and use generalist models such as LLMs. Due to its flexibility, prompting has been widely used as a generic method for NLP tasks (Schick & Schütze, 2021; Brown et al., 2020; Sanh et al., 2022). However, LLMs require careful prompt engineering, either manually (Reynolds & McDonell, 2021) or automatically (Gao et al., 2021; Shin et al., 2020), as models do not seem to understand the prompts in the same way a human would (Webson & Pavlick, 2021; Lu et al., 2021). Though many successful 2\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will focus on Cognitive Search and the following topics:\n",
    "1. Creating an index client\n",
    "2. Defining the index fields with necessary attributes\n",
    "3. Creating a semantic configuration\n",
    "4. Loading our index with the document pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x7e7dfcab46d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")   \n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "index_name = \"research-paper-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-index created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 179 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see Azure Cognitive Search in action! We can retrive the most relevant documents out of all the ones that we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is automated prompt engineering?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['document_name'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table 25: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>likely that models could arrive at the correct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5 Size Prompt Length Trainable Parameters Tot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>matches, the best individual prompt. 7 Interpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F Appendix: Input/Output Examples Table 13: Ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 23: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at every transformer layer. This is akin to le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks\n",
       "0  Table 24: Few-shot exemplars for full chain of...\n",
       "1  Table 25: Few-shot exemplars for full chain of...\n",
       "2  Self-Consistency Improves Chain of Thought Rea...\n",
       "3  likely that models could arrive at the correct...\n",
       "4  T5 Size Prompt Length Trainable Parameters Tot...\n",
       "5  matches, the best individual prompt. 7 Interpr...\n",
       "6  F Appendix: Input/Output Examples Table 13: Ex...\n",
       "7  Table 23: Few-shot exemplars for full chain of...\n",
       "8  at every transformer layer. This is akin to le...\n",
       "9  Self-Consistency Improves Chain of Thought Rea..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the most relevant documents, let us create embeddings for all the page chunks. This will help us find the most similar documents to our given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                input=[text],\n",
    "                model=model\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            break;\n",
    "        except Exception as e:\n",
    "            # Handle rate limiting and other errors\n",
    "            if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                count+=1\n",
    "                #print(f'Rate Limit Error Count: {count}')\n",
    "                sleep(2)\n",
    "            else:\n",
    "                raise e            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, model = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[0.013752952, 0.012547761, 0.0302744, -0.00447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table 25: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[-0.0046408083, 0.011499113, -0.005062398, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "      <td>[0.012865214, 0.023359789, 0.007188161, -0.032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>likely that models could arrive at the correct...</td>\n",
       "      <td>[0.028534416, 0.0068504177, 0.021387327, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5 Size Prompt Length Trainable Parameters Tot...</td>\n",
       "      <td>[-0.033432808, 0.007821185, 0.026300108, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>matches, the best individual prompt. 7 Interpr...</td>\n",
       "      <td>[-0.011787865, -0.011878273, 0.018540679, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F Appendix: Input/Output Examples Table 13: Ex...</td>\n",
       "      <td>[-0.0065887556, 0.029776651, 0.009727606, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 23: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[-0.007411479, 0.0077118557, 0.013974066, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at every transformer layer. This is akin to le...</td>\n",
       "      <td>[-0.021121152, -0.0059559154, 0.018654246, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "      <td>[0.0022435007, 0.018677488, 0.0099718785, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  Table 24: Few-shot exemplars for full chain of...   \n",
       "1  Table 25: Few-shot exemplars for full chain of...   \n",
       "2  Self-Consistency Improves Chain of Thought Rea...   \n",
       "3  likely that models could arrive at the correct...   \n",
       "4  T5 Size Prompt Length Trainable Parameters Tot...   \n",
       "5  matches, the best individual prompt. 7 Interpr...   \n",
       "6  F Appendix: Input/Output Examples Table 13: Ex...   \n",
       "7  Table 23: Few-shot exemplars for full chain of...   \n",
       "8  at every transformer layer. This is akin to le...   \n",
       "9  Self-Consistency Improves Chain of Thought Rea...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.013752952, 0.012547761, 0.0302744, -0.00447...  \n",
       "1  [-0.0046408083, 0.011499113, -0.005062398, -0....  \n",
       "2  [0.012865214, 0.023359789, 0.007188161, -0.032...  \n",
       "3  [0.028534416, 0.0068504177, 0.021387327, -0.03...  \n",
       "4  [-0.033432808, 0.007821185, 0.026300108, -0.01...  \n",
       "5  [-0.011787865, -0.011878273, 0.018540679, -0.0...  \n",
       "6  [-0.0065887556, 0.029776651, 0.009727606, -0.0...  \n",
       "7  [-0.007411479, 0.0077118557, 0.013974066, -0.0...  \n",
       "8  [-0.021121152, -0.0059559154, 0.018654246, -0....  \n",
       "9  [0.0022435007, 0.018677488, 0.0099718785, -0.0...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matches, the best individual prompt. 7 Interpr...</td>\n",
       "      <td>[-0.011787865, -0.011878273, 0.018540679, -0.0...</td>\n",
       "      <td>0.809985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at every transformer layer. This is akin to le...</td>\n",
       "      <td>[-0.021121152, -0.0059559154, 0.018654246, -0....</td>\n",
       "      <td>0.804920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Rea...</td>\n",
       "      <td>[0.0022435007, 0.018677488, 0.0099718785, -0.0...</td>\n",
       "      <td>0.766031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  matches, the best individual prompt. 7 Interpr...   \n",
       "1  at every transformer layer. This is akin to le...   \n",
       "2  Self-Consistency Improves Chain of Thought Rea...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.011787865, -0.011878273, 0.018540679, -0.0...      0.809985  \n",
       "1  [-0.021121152, -0.0059559154, 0.018654246, -0....      0.804920  \n",
       "2  [0.0022435007, 0.018677488, 0.0099718785, -0.0...      0.766031  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df\n",
    "\n",
    "query_embedding = get_embedding(query, model=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
      "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
      "\n",
      "User Query: ```What is automated prompt engineering?```\n",
      "List of Extracted Pages: ```['matches, the best individual prompt. 7 Interpretability An ideally interpretable prompt would consist of natural language that clearly describes the task at hand, explicitly asks the model for some result or action, and makes it easy to understand why the prompt elicited such behavior from the model. As prompt tuning works in the continuous em- bedding space rather than the discrete token space, interpreting prompts becomes more difficult. To test the interpretability of our learned soft prompts, we compute the nearest neighbors to each prompt token from the frozen model’s vocabulary. We use cosine distance between the vocabulary embedding vector and the prompt token representation as the similarity metric. We observe that for a given learned prompt to- ken, the top-5 nearest neighbors form tight seman- tic clusters. For example, we see lexically similar clusters such as { Technology / technology / Tech- nologies / technological / technologies }, as well as more diverse but still strongly related clusters such as { entirely / completely / totally / altogether / 100% }. The nature of these clusters suggests that the prompts are in fact learning “word-like” repre- sentations. We found that random vectors drawn from the embedding space do not show this sort of semantic clustering. When initializing the prompts using the “class- label” strategy, we often find that the class labels persist through training. Specifically, if a prompt token is initialized to a given label, that label is often among the learned token’s nearest neighbors after tuning. When initializing with the “Random Uniform” or “Sampled Vocab” methods, the class labels can also be found in the nearest neighbors of the prompts; however they tend to appear as neighbors to multiple prompt tokens. This suggests that the model is learning to store the expected output classes in the prompts as reference, and initializing the prompt to outputs classes makes this easier and more centralized. When examining longer prompts (e.g. size 100), we often find several prompt tokens with the same nearest neighbors. This suggests there is either excess capacity in the prompt, or that the lack of sequential structure in the prompt representation makes it difficult for the model to localize informa- tion to a specific position. While the learned prompts taken as sequences show little interpretability, we do observe a high frequency of words like science, technology and engineering as the nearest neighbors for prompts trained on the BoolQ dataset and approximately 20% of the questions are in the “Nature/Science” category. While more investigation is needed, this suggests that one role of the prompt may be to prime the model to interpret inputs in a specific domain or context (e.g. “scientific”). 8 Conclusion In this paper, we showed that prompt tuning is a competitive technique for adapting frozen pre- trained language models to downstream tasks. On the popular SuperGLUE benchmark, its task perfor- mance rivals that of traditional model tuning, with the gap vanishing as model size increases. On zero- shot domain transfer, we found that prompt tuning leads to improved generalization. This plausibly in- dicates that freezing general-purpose language un- derstanding parameters and restricting downstream learning to a lightweight parameter footprint can help to avoid overfitting to a specific domain. Beyond task quality metrics, we discussed the appeal of moving to frozen pre-trained models in terms of storage and serving costs. This move enables both efficient multi-task serving, as well as efficient high-performing prompt ensembling. Looking forward, we believe that factoring out task-defining parameters as distinct from general language-modeling parameters is an exciting step that opens up many avenues for new research. Acknowledgements We thank Lucas Dixon, Waleed Ammar, Slav Petrov and Sebastian Ruder for comments on an earlier draft, and the following people for helpful discussion: Colin Raffel, Adam Roberts, and Noam Shazeer. We thank Linting Xue for help with the LM adaptation training. References Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognis- ing textual entailment, volume 6, pages 6–4. Venice. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC.', 'at every transformer layer. This is akin to learning transformer activations that are fixed across exam- ples at every network layer. In contrast, prompt tuning uses a single prompt representation that is prepended to the embedded input. Beyond re- quiring fewer parameters, our approach allows the transformer to update the intermediate-layer task representations, as contextualized by an input ex- ample. Their work builds on GPT-2 (Radford et al., 2019) and BART (Lewis et al., 2020), while ours fo- cuses on T5 and examines changes in performance and robustness to design choices as model size in- creases. When using BART, prefix tuning includes prefixes on both the encoder and decoder network, while prompt tuning only requires prompts on the encoder. Li and Liang (2021) also rely on a repa- rameterization of the prefix to stabilize learning, which adds a large number of parameters during training, whereas our configuration does not re- quire this reparameterization and is robust across SuperGLUE tasks and model sizes. Hambardzumyan et al. (2021) propose “WARP”, where prompt parameters are added to the input layer. This method works with masked language models, relying on a [MASK] token and a learn- able output layer to project the mask to class logits. This formulation restricts the model to producing a single output, limiting it to classification. Prompt tuning does not require any changes to the input or a task-specific head. The performance of prompt tuning is also considerably closer to the strong per- formance of model tuning. Liu et al. (2021) propose “P-tuning” where learn- able continuous prompts are interleaved throughout the embedded input, using patterns based on human design. Our approach removes this complication by simply prepending the prompt to the input. To achieve strong SuperGLUE results, P-tuning has to be used in conjunction with model tuning, that is, models jointly update both the prompt and the main model parameters, whereas our approach keeps the original language model frozen.10 Qin and Eisner (2021) use “soft words” to learn prompts to extract knowledge from pre-trained LMs. Prompts are positioned in relation to the input based on hand-designed prompt prototypes, and a learned ∆ i parameter is included for each layer, so parameter cost scales with model depth. 10As another difference, P-tuning requires the addition of “anchor” tokens in the input (e.g. a question mark following the hypothesis in the RTE task) to achieve strong performance, while prompt tuning leaves inputs untouched. Dataset Domain - Model Prompt ∆ SQuAD Wiki 94.9 ±0.2 94.8 ±0.1 −0.1 TextbookQA Book 54.3 ±3.7 66.8 ±2.9 +12.5 BioASQ Bio 77.9 ±0.4 79.1 ±0.3 +1.2 RACE Exam 59.8 ±0.6 60.7 ±0.5 +0.9 RE Wiki 88.4 ±0.1 88.8 ±0.2 +0.4 DuoRC Movie 68.9 ±0.7 67.7 ±1.1 −1.2 DROP Wiki 68.9 ±1.7 67.1 ±1.9 −1.8 Table 1: F1 mean and stddev for models trained on SQuAD and evaluated on out-of-domain datasets from the MRQA 2019 shared task. Prompt tuning tends to give stronger zero-shot performance than model tun- ing, especially on datasets with large domain shifts like TextbookQA. Logeswaran et al. (2020) use a learnable prepended token to adapt transformer models to var- ious tasks, but focus on small synthetic datasets de- signed to accommodate a compositional task repre- sentation, as opposed to larger real-world datasets. Their base models are small transformers trained from scratch jointly with the task representations, whereas we keep the base model frozen and inves- tigate scaling laws using larger transformers. More generally, work on task prompts is closely aligned with work on “adapters” (Rebuffi et al., 2017; Houlsby et al., 2019), small bottleneck lay- ers inserted between frozen pre-trained network layers. Adapters offer another means of reduc- ing task-specific parameters, with Houlsby et al. (2019) achieving GLUE performance close to full model tuning when freezing BERT-Large and only adding 2–4% additional parameters. Pfeiffer et al. (2020) use multiple adapters in a multilingual con- text to explicitly separate language understanding from task specification, similar to our approach. A core difference between adapters and prompt tun- ing is how the approaches change model behavior. Adapters modify the actual function that acts on the input representation, parameterized by the neural network, by allowing the rewriting of activations at any given layer. Prompt tuning modifies behavior by leaving the function fixed and adding new in- put representations that can affect how subsequent input is processed. 5 Resilience to Domain Shift By freezing the core language model parameters, prompt tuning prevents the model from modify- ing its general understanding of language. Instead, prompt representations indirectly modulate the rep- resentation of the input. This reduces the model’s ability to overfit to a dataset by memorizing spe-', 'Self-Consistency Improves Chain of Thought Reasoning in Language Models GSM8K accuracy Different sets of prompts (x40) 58.9 Prompt-permutation (x40) 59.6 Self-consistency (x40) 74.4 Self-consistency + different sets of prompts (x40) 75.4 Self-consistency + prompt-permutation (x40) 73.8 Table 11: Combining self-consistency with other ensembling strategies. permutations to compare with self-consistency with 40 paths, all experiments are based on PaLM- 540B). Question Greedy Decode Sampled Path 1 Sampled Path 2 [MultiArith] Sam had 10 socks. If he threw away 3 old ones that didn’t fit and bought 36 new ones, how many socks would he have? Sam had 10 socks. He threw away 3 old ones that didn’t fit. He bought 36 new ones. That means he has 10 + 36 - 3 = 41 socks. The answer is 41. Sam had 10 socks. He threw away 3 old ones. So now he has 7 left. Then he bought 36 new ones. So now he has 7 + 36 = 43 socks. The answer is 43. He had 10 socks. He threw away 3 old ones that didn’t fit. He bought 36 new ones. So in total he has 10 - 3 + 36 = 43 socks. The answer is 43. [SVAMP] In a school there are 697 girls and the rest are boys. If there are 228 more girls than boys. How many boys are there in that school? There are 697 girls and the rest are boys. That means there are 697 + 228 = 925 girls. The rest are boys. So the number of boys is 925 - 697 = 228. The answer is 228. The number of boys is the difference between 697 and 228. This is 697 - 228 = 469. The answer is 469. In the beginning there are 697 girls and the rest are boys. So, the number of boys must be 697 - 228 = 469 boys. The answer is 469. [AQuA] There are 10 oranges in a basket. Find the no. of ways in which 2 oranges are chosen from the basket? (a) 45 (b) 90 (c) 120 (d) 150 (e) 180 There are 10 oranges in a bas- ket. So there are 10 ways in which 2 oranges are chosen from the basket. There are 10 oranges in a basket. We need to choose 2 oranges from the basket. So the number of ways are 10C2. So the answer is (a). There are 10 oranges in the bas- ket. If you choose 2 oranges, you have 10C2 = 45 ways of choosing 2 oranges. So the answer is (a). [CommonsenseQA] The man laid on the soft moss and looked up at the trees, where was the man? (a) niagra falls (b) forest (c) waterfall (d) ground (e) tree The answer must be something that is soft and mossy. Of the above choices, only ground is soft and mossy. So the answer is (d). The answer must be somewhere where moss is found. Of the above choices, only forests are known for having moss. So the answer is (b). The answer must be a place where the man is laying down and look- ing up at trees. Of the above choices, only forest is a place where people lay down and look at trees. So the answer is (b). [ARC] Which of these will most likely harm a habitat? (a) plant- ing trees. (b) water pollution. (c) rainfall. (d) sunlight. Planting trees will not harm a habitat. So the answer is (a). The answer is (b), water pollution, because it will most likely harm the habi- tat. The other answers are not likely to harm the habi- tat. The answer is (b), water pollution. Trees are good for a habitat, rain- fall is necessary for a habitat, and sunlight is necessary for a habitat. So the answer is (b). [ARC-challenge] Which pro- cess in the carbon cycle takes the longest to complete? (a) emis- sion of waste. (b) respiration in animals. (c) photosynthesis in plants. (d) formation of fossil fu- els. The process of photosynthesis in plants takes the longest to complete. So the answer is (c). The formation of fossil fu- els takes the longest time to complete. So the an- swer is (d). The process that takes the longest time to complete is the forma- tion of fossil fuels, which hap- pens over millions of years. So the answer is (d). Table 12: Additional examples where self-consistency helps repair the errors over greedy decode on LaMDA-137B. Two sampled reasoning paths that are consistent with the ground truth are shown. A.2 FULL SETS OF PROMPTS We list the full details of the prompts used for two newly-introduced datasets, AQUA-RAT (Ling et al., 2017) and AI2 Reasoning Challenge (ARC) (Clark et al., 2018), where we manually composed the example chains of thought in this paper, in Table 14 and Table 15, respectively. As additional information, we also list the exact set of prompts used for all arithmetic reasoning tasks in Table 17, since there are multiple sets of prompts introduced in Wei et al. (2022). The prompts for CommonsenseQA and StrategyQA are the same as used in Wei et al. (2022). We provide the exact prompts used for common NLP tasks in the following tables as well, including NLI and Closed-Book Question-Answering tasks.']```\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "User Query: ```{query}```\n",
    "List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering refers to the process of designing and optimizing prompts for language models in a systematic and automated manner. This involves creating prompts that effectively guide the model to perform specific tasks or generate desired outputs without manually crafting each prompt. The concept is closely related to prompt tuning, which involves adjusting prompts in the continuous embedding space rather than the discrete token space. This makes interpreting the prompts more challenging, as they do not consist of straightforward natural language instructions.\n",
      "\n",
      "The process of automated prompt engineering can include techniques such as initializing prompts with specific strategies (e.g., \"class-label\" strategy) and refining them through training to ensure they align closely with the desired task outcomes. The goal is to create prompts that can prime the model to interpret inputs within a specific domain or context, such as scientific or technological fields, as observed in the semantic clustering of learned prompts.\n",
      "\n",
      "Automated prompt engineering is part of a broader effort to adapt pre-trained language models to various downstream tasks efficiently. It allows for the modification of model behavior by adding new input representations, which can affect how subsequent inputs are processed, without altering the core language model parameters. This approach helps prevent overfitting to specific datasets and supports better generalization across different domains. Additionally, it reduces the need for extensive parameter updates, making it a cost-effective and scalable solution for leveraging large language models in diverse applications.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt, chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"])\n",
    "    embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, model = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, model=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "    Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "    User Query: ```{query}```\n",
    "    List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt, chat_model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering involves techniques that modify or enhance the input to a language model to improve its performance on specific tasks without altering the model's core parameters. Several methods have been developed to achieve this, each with its unique approach and benefits:\n",
      "\n",
      "1. **Prompt Tuning**: This method involves prepending a learnable prompt to the input data. Unlike traditional model tuning, which updates the entire model's parameters, prompt tuning keeps the language model's parameters frozen and only adjusts the prompt embeddings. This approach is highly parameter-efficient, requiring less than 0.01% task-specific parameters for large models, and is particularly effective in scenarios with significant domain shifts, such as moving from one dataset to another.\n",
      "\n",
      "2. **Prefix Tuning**: Proposed by Li and Liang (2021), this technique involves learning a sequence of prefixes that are added to both the encoder and decoder networks in models like BART. It requires more parameters during training but is efficient during inference, using only 0.1–1% task-specific parameters.\n",
      "\n",
      "3. **WARP (Weighted Average Representation of Prompts)**: This method adds prompt parameters to the input layer and is designed for masked language models. It uses a [MASK] token and a learnable output layer to project the mask to class logits, which restricts the model to producing a single output, typically for classification tasks.\n",
      "\n",
      "4. **P-tuning**: Introduced by Liu et al. (2021), P-tuning involves interleaving learnable continuous prompts throughout the embedded input, using patterns based on human design. It requires joint tuning of both the prompt and the main model parameters to achieve strong results, especially in tasks like SuperGLUE.\n",
      "\n",
      "5. **Soft Words**: Qin and Eisner (2021) use \"soft words\" to learn prompts that extract knowledge from pre-trained language models. These prompts are positioned relative to the input based on hand-designed prototypes, with a learned parameter for each layer, making the parameter cost scale with model depth.\n",
      "\n",
      "Overall, automated prompt engineering focuses on modifying the input representation to guide the model's behavior without altering its fundamental understanding of language. This approach not only reduces the risk of overfitting but also enhances the model's adaptability to new tasks and domains.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"How does automated prompt engineering work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning is a technique used to adapt pre-trained language models to specific downstream tasks by learning \"soft prompts.\" Unlike traditional model tuning, which involves adjusting all the parameters of a model, prompt tuning keeps the core model parameters frozen and only learns a small set of additional parameters that are prepended to the input as a prompt. This approach is parameter-efficient, requiring significantly fewer parameters than full model tuning, and allows for the reuse of a single pre-trained model across multiple tasks.\n",
      "\n",
      "The key idea behind prompt tuning is to condition the model on task-specific information by adding a learned prompt to the input data. This prompt is trained end-to-end using backpropagation, allowing it to incorporate signals from labeled examples. The method is particularly effective as the size of the model increases, becoming competitive with full model tuning for large models with billions of parameters.\n",
      "\n",
      "Prompt tuning offers several advantages:\n",
      "1. **Parameter Efficiency**: It requires only a small number of additional parameters per task, making it much more efficient than model tuning, which requires a separate copy of the entire model for each task.\n",
      "2. **Robustness to Domain Shift**: By keeping the general language understanding parameters fixed, prompt tuning can achieve better resilience to changes in the domain of the input data.\n",
      "3. **Ease of Use**: It simplifies the adaptation process by avoiding the need for task-specific model copies and allowing for mixed-task inference using a single model.\n",
      "4. **Prompt Ensembling**: The technique allows for the learning of multiple prompts for the same task, which can improve performance and is more efficient than traditional model ensembling.\n",
      "\n",
      "Overall, prompt tuning is a powerful method for leveraging large pre-trained language models for specific tasks while maintaining efficiency and robustness.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is prompt tuning?\", 10)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
